# Assignment 3: Use recurrent neural networks to build a transliteration system.
----------------------------------------------------
In this project we implement a model sequence to sequence learning problems using Recurrent Neural Networks, compare different cells such as vanilla RNN, LSTM and GRU, implement attention networks to overcome the limitations of vanilla seq2seq model and visualise the interactions between different components in an RNN based model. We use wandb for hyper parameter configuration using the validation dataset and visualisation of test data. We have performed a large number of experiments to make meaningful inferences and get to our best model.

# Set up and Installation: #
----------------------------------------------------
Both vanilla_seq2seq and seq_2_seq_with_attention has been implented in Google Colab.</br>
`git clone https://github.com/utsavdey/cs6910_assignment3.git`

Visit [here](https://github.com/utsavdey/cs6910_assignment3/tree/main/vanilla_seq2seq) to know more about vanilla_seq2seq without attention.</br>
Visit [here](https://github.com/utsavdey/cs6910_assignment3/tree/main/seq2seq_with_attention) to know more about seq2seq with attention network.
